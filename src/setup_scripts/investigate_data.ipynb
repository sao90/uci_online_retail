{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path to import src module\n",
    "sys.path.append(str(Path().resolve().parent.parent))\n",
    "\n",
    "import pandas as pd\n",
    "from darts import TimeSeries\n",
    "\n",
    "\n",
    "from src.modules.model_handling.model_catalogue import MODEL_CATALOGUE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_covariates_df = pd.read_parquet(\"../../data/pipeline_runs/future_covariates.parquet\")\n",
    "past_covariates_df = pd.read_parquet(\"../../data/pipeline_runs/past_covariates.parquet\")\n",
    "target_train_df = pd.read_parquet(\"../../data/pipeline_runs/train_targets_daily.parquet\")\n",
    "target_test_df = pd.read_parquet(\"../../data/pipeline_runs/test_targets_daily.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_covariates_columns = [\"is_holiday\"]\n",
    "past_covariates_columns = [\"num_transactions\", \"num_unique_customers\", \n",
    "                           \"num_unique_articles\", \"avg_basket_size\", \n",
    "                           \"avg_unit_price\"]\n",
    "target_columns = [\"Quantity\"]\n",
    "time_index_column = \"InvoiceDate\"\n",
    "\n",
    "future_covariates = TimeSeries.from_dataframe(\n",
    "    future_covariates_df, \n",
    "    time_col=time_index_column, \n",
    "    value_cols=future_covariates_columns,\n",
    "    fill_missing_dates=True,\n",
    "    fillna_value=0, \n",
    "    freq='D',\n",
    ")\n",
    "past_covariates = TimeSeries.from_dataframe(\n",
    "    past_covariates_df, \n",
    "    time_col=time_index_column,\n",
    "    value_cols=past_covariates_columns,\n",
    "    fill_missing_dates=True,\n",
    "    fillna_value=0,\n",
    "    freq='D',\n",
    ")\n",
    "target_train = TimeSeries.from_dataframe(\n",
    "    target_train_df,\n",
    "    time_col=time_index_column,\n",
    "    value_cols=target_columns,\n",
    "    fill_missing_dates=True,\n",
    "    fillna_value=0,\n",
    "    freq='D',\n",
    ")\n",
    "target_test = TimeSeries.from_dataframe(\n",
    "    target_test_df,\n",
    "    time_col=time_index_column,\n",
    "    value_cols=target_columns,\n",
    "    fill_missing_dates=True,\n",
    "    fillna_value=0,\n",
    "    freq='D',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Configuration Parameters\n",
    "\n",
    "Set model and evaluation parameters here to run experiments with different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.metrics import rmse, wmape\n",
    "# Model configuration\n",
    "MODEL_KEY = \"random_forest_7777_cyclic_day_month_scaled\"  # Change this to test different models\n",
    "\n",
    "# Backtest configuration\n",
    "FORECAST_HORIZON = 7  # Days to forecast ahead\n",
    "BACKTEST_STRIDE = 7   # Days between backtest evaluations\n",
    "BACKTEST_START = 0.7  # Start backtesting at 70% through training data\n",
    "BACKTEST_RETRAIN = True  # Whether to retrain at each backtest step (True = slower but more realistic)\n",
    "METRICS = [rmse, wmape]\n",
    "# Test set configuration\n",
    "TEST_FORECAST_DAYS = 30  # How many days to predict on test set\n",
    "\n",
    "print(f\"Model: {MODEL_KEY}\")\n",
    "print(f\"Backtest: {FORECAST_HORIZON}-day horizon, stride={BACKTEST_STRIDE}, retrain={BACKTEST_RETRAIN}\")\n",
    "print(f\"Test forecast: {TEST_FORECAST_DAYS} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Model Evaluation Strategy\n",
    "\n",
    "**Backtest (on training data)**: \n",
    "- Simulates real-world forecasting by repeatedly training and testing on historical data\n",
    "- Used to validate model performance and tune hyperparameters\n",
    "- Safe to run multiple times during development\n",
    "\n",
    "**Test Set Evaluation (final check)**:\n",
    "- Holdout data that model has never seen\n",
    "- Used ONLY ONCE for final unbiased performance assessment\n",
    "- Touching it multiple times = data leakage\n",
    "\n",
    "**Proper workflow**:\n",
    "1. Backtest on training data → tune model\n",
    "2. Final evaluation on test set → report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Backtest on training data (for model validation & tuning)\n",
    "from darts.metrics import rmse, mae, smape, mase, wmape\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: BACKTEST ON TRAINING DATA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {MODEL_KEY}\")\n",
    "print(\"Purpose: Validate model can generalize to unseen time periods\")\n",
    "print()\n",
    "\n",
    "model = MODEL_CATALOGUE[MODEL_KEY]()\n",
    "\n",
    "# Get the model configuration to understand what covariates it needs\n",
    "model_config = model.model_params if hasattr(model, 'model_params') else {}\n",
    "print(f\"Model config: {model_config}\")\n",
    "print()\n",
    "\n",
    "model.fit(\n",
    "    series=target_train,\n",
    "    past_covariates=past_covariates,\n",
    "    future_covariates=future_covariates,\n",
    ")\n",
    "\n",
    "# Backtest on training data - simulates forecasting at multiple points in history\n",
    "print(\"Running backtest...\")\n",
    "backtest_metrics = model.backtest(\n",
    "    series=target_train,  # Use TRAINING data for backtest\n",
    "    past_covariates=past_covariates,\n",
    "    future_covariates=future_covariates,\n",
    "    forecast_horizon=FORECAST_HORIZON,\n",
    "    stride=BACKTEST_STRIDE,\n",
    "    retrain=BACKTEST_RETRAIN,\n",
    "    last_points_only=True,  # Returns scalar metric, not forecast series\n",
    "    metric=METRICS,\n",
    "    start=BACKTEST_START,\n",
    ")\n",
    "\n",
    "print(f\"Training Backtest RMSE ({FORECAST_HORIZON}-day horizon): {backtest_metrics[0]:.2f}\")\n",
    "print(f\"Training Backtest WMAPE ({FORECAST_HORIZON}-day horizon): {backtest_metrics[1]:.2f}\")\n",
    "print(\"This shows how well the model performs on historical data it was trained on\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = {METRICS[i]: score for i, score in enumerate(backtest_metrics)}\n",
    "import json\n",
    "print(\n",
    "json.dumps(score_dict, indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Final evaluation on test set (ONLY ONCE)\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 2: FINAL EVALUATION ON TEST SET (HOLDOUT DATA)\")\n",
    "print(\"=\"*60)\n",
    "print(\"Purpose: Unbiased assessment of model performance on unseen data\")\n",
    "print()\n",
    "\n",
    "# Predict on test set\n",
    "forecast = model.predict(\n",
    "    n=TEST_FORECAST_DAYS,\n",
    "    past_covariates=past_covariates,\n",
    "    future_covariates=future_covariates\n",
    ")\n",
    "\n",
    "# Calculate metrics (using zero-friendly percentage metrics)\n",
    "test_rmse = rmse(target_test, forecast)\n",
    "test_mae = mae(target_test, forecast)\n",
    "test_wmape = wmape(target_test, forecast)  # Weighted MAPE - handles zeros, most interpretable\n",
    "test_smape = smape(target_test, forecast)  # Symmetric MAPE - handles zeros\n",
    "test_mase = mase(target_test, forecast, insample=target_train)  # Mean Absolute Scaled Error\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(f\"  RMSE:   {test_rmse:.2f} (root mean squared error)\")\n",
    "print(f\"  MAE:    {test_mae:.2f} (mean absolute error)\")\n",
    "print(f\"  WMAPE:  {test_wmape:.2f}% (weighted MAPE, handles zeros)\")\n",
    "print(f\"  sMAPE:  {test_smape:.2f}% (symmetric MAPE, 0-200% range)\")\n",
    "print(f\"  MASE:   {test_mase:.2f} (vs naive forecast, <1 is good)\")\n",
    "print()\n",
    "print(\"⚠️  This is the TRUE model performance - only check this once!\")\n",
    "print(\"   If you iterate based on test results, you're leaking information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the data scale and error context\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA SCALE & ERROR CONTEXT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nTarget Variable Statistics:\")\n",
    "print(f\"  Train mean: {target_train.values().mean():.2f}\")\n",
    "print(f\"  Train std:  {target_train.values().std():.2f}\")\n",
    "print(f\"  Train range: [{target_train.values().min():.0f}, {target_train.values().max():.0f}]\")\n",
    "print(f\"  Zero values in train: {(target_train.values() == 0).sum()} / {len(target_train)}\")\n",
    "print(f\"\\n  Test mean: {target_test.values().mean():.2f}\")\n",
    "print(f\"  Test std:  {target_test.values().std():.2f}\")\n",
    "print(f\"  Zero values in test: {(target_test.values() == 0).sum()} / {len(target_test)}\")\n",
    "\n",
    "print(f\"\\nForecast Statistics:\")\n",
    "print(f\"  Forecast mean: {forecast.values().mean():.2f}\")\n",
    "print(f\"  Forecast std:  {forecast.values().std():.2f}\")\n",
    "\n",
    "print(f\"\\nError in Context:\")\n",
    "print(f\"  RMSE as % of test mean: {(test_rmse / target_test.values().mean()) * 100:.1f}%\")\n",
    "print(f\"  MAE as % of test mean:  {(test_mae / target_test.values().mean()) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize forecast vs actual\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "target_test.plot(label=\"Actual Test Data\")\n",
    "forecast.plot(label=\"30-Day Forecast\")\n",
    "plt.title(\"Forecast vs Actual Test Data\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Quantity\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check forecast statistics\n",
    "print(\"\\n=== Forecast Statistics ===\")\n",
    "print(f\"Forecast mean: {forecast.values().mean():.2f}\")\n",
    "print(f\"Forecast std: {forecast.values().std():.2f}\")\n",
    "print(f\"Forecast min: {forecast.values().min():.2f}\")\n",
    "print(f\"Forecast max: {forecast.values().max():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uci_online_retail-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
